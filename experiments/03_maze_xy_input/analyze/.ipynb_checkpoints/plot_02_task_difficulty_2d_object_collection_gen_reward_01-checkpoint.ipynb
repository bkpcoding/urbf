{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e19a3373525b4275b47f1932288403f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ExperimentDataLoaderWidget(children=(Box(children=(Button(description='Update Descriptions', layout=Layout(heiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import the experiment utilities package\n",
    "import exputils as eu\n",
    "import numpy as np\n",
    "\n",
    "# define what data should be loaded and some extra statistics that should be computed                \n",
    "loader_config = eu.AttrDict(\n",
    "    load_experiment_data_function = eu.AttrDict(\n",
    "        pre_allowed_data_filter = [\n",
    "            'rollout_ep_',\n",
    "        ],\n",
    "    )\n",
    ")       \n",
    "                \n",
    "# create an experiment data loader, by default it will load data from '../experiments'\n",
    "experiment_data_loader = eu.gui.jupyter.ExperimentDataLoaderWidget(config=loader_config)\n",
    "display(experiment_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "def average_reward_over_time(file):\n",
    "    # read the data from the .npy file\n",
    "    data = np.load(file, allow_pickle = True)\n",
    "    # min-max normalize the data\n",
    "    data = (data + 200) / (200 - 11)\n",
    "    # calculate the area under the curve of data against time\n",
    "    area = np.trapz(data, dx = 1)\n",
    "    # calculate the normalized area under the curve\n",
    "    norm_area = area / data.shape[0]\n",
    "    # map it to [0,1], given the range of the data is [-12, -200]\n",
    "    return norm_area\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111013\n",
      "nan (nan)\n",
      "111113\n",
      "0.7874 (0.129)\n",
      "111313\n",
      "nan (nan)\n",
      "111413\n",
      "nan (nan)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sagar/anaconda3/envs/mini/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/sagar/anaconda3/envs/mini/lib/python3.8/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/sagar/anaconda3/envs/mini/lib/python3.8/site-packages/numpy/core/_methods.py:265: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/home/sagar/anaconda3/envs/mini/lib/python3.8/site-packages/numpy/core/_methods.py:223: RuntimeWarning: invalid value encountered in divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "/home/sagar/anaconda3/envs/mini/lib/python3.8/site-packages/numpy/core/_methods.py:257: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "    # URBF Level 3\n",
    "    average_reward_list = []\n",
    "    experiment_ids = [\n",
    "        \"111013\",\n",
    "        \"111113\",\n",
    "        \"111313\",\n",
    "        \"111413\",\n",
    "    ]\n",
    "    for ids in experiment_ids:\n",
    "        average_reward_list = []\n",
    "        for j in range(9):\n",
    "            file = '../experiments/experiment_' + str(ids) + '/repetition_00000' + str(j) + '/data/rollout_ep_rew_mean.npy'\n",
    "            try:\n",
    "                average_reward_list.append(average_reward_over_time(file))\n",
    "            except:\n",
    "                continue\n",
    "        print(ids)\n",
    "        print(\"%.4f (%.3f)\" % (np.mean(average_reward_list), np.std(average_reward_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../experiments/experiment_020001/repetition_000000/data/rollout_ep_rew_mean.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m9\u001b[39m):\n\u001b[1;32m     12\u001b[0m     file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../experiments/experiment_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(ids) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/repetition_00000\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(j) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/data/rollout_ep_rew_mean.npy\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 13\u001b[0m     average_reward_list\u001b[38;5;241m.\u001b[39mappend(\u001b[43maverage_reward_over_time\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(ids)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%.4f\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m%.3f\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (np\u001b[38;5;241m.\u001b[39mmean(average_reward_list), np\u001b[38;5;241m.\u001b[39mstd(average_reward_list)))\n",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36maverage_reward_over_time\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maverage_reward_over_time\u001b[39m(file):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# read the data from the .npy file\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# min-max normalize the data\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     data \u001b[38;5;241m=\u001b[39m (data \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m200\u001b[39m) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m11\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/mini/lib/python3.8/site-packages/numpy/lib/npyio.py:390\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    388\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 390\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    391\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../experiments/experiment_020001/repetition_000000/data/rollout_ep_rew_mean.npy'"
     ]
    }
   ],
   "source": [
    "    # MLP Level 1\n",
    "    average_reward_list = []\n",
    "    experiment_ids = [\n",
    "        \"020001\",\n",
    "        \"020301\",\n",
    "        \"020401\",\n",
    "        \"020501\",\n",
    "    ]\n",
    "    for ids in experiment_ids:\n",
    "        average_reward_list = []\n",
    "        for j in range(9):\n",
    "            file = '../experiments/experiment_' + str(ids) + '/repetition_00000' + str(j) + '/data/rollout_ep_rew_mean.npy'\n",
    "            average_reward_list.append(average_reward_over_time(file))\n",
    "        print(ids)\n",
    "        print(\"%.4f (%.3f)\" % (np.mean(average_reward_list), np.std(average_reward_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111011\n",
      "0.9264 (0.026)\n",
      "111111\n",
      "0.8638 (0.120)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../experiments/experiment_111311/repetition_000000/data/rollout_ep_rew_mean.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m9\u001b[39m):\n\u001b[1;32m     11\u001b[0m     file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../experiments/experiment_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(ids) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/repetition_00000\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(j) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/data/rollout_ep_rew_mean.npy\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 12\u001b[0m     average_reward_list\u001b[38;5;241m.\u001b[39mappend(\u001b[43maverage_reward_over_time\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(ids)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%.4f\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m%.3f\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (np\u001b[38;5;241m.\u001b[39mmean(average_reward_list), np\u001b[38;5;241m.\u001b[39mstd(average_reward_list)))\n",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36maverage_reward_over_time\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maverage_reward_over_time\u001b[39m(file):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# read the data from the .npy file\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# min-max normalize the data\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     data \u001b[38;5;241m=\u001b[39m (data \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m200\u001b[39m) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m11\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/mini/lib/python3.8/site-packages/numpy/lib/npyio.py:390\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    388\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 390\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    391\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../experiments/experiment_111311/repetition_000000/data/rollout_ep_rew_mean.npy'"
     ]
    }
   ],
   "source": [
    "    # URBF level 1\n",
    "    average_reward_list = []\n",
    "    experiment_ids = [\n",
    "        \"111011\",\n",
    "        \"111111\",\n",
    "        \"111311\",\n",
    "    ]\n",
    "    for ids in experiment_ids:\n",
    "        average_reward_list = []\n",
    "        for j in range(9):\n",
    "            file = '../experiments/experiment_' + str(ids) + '/repetition_00000' + str(j) + '/data/rollout_ep_rew_mean.npy'\n",
    "            average_reward_list.append(average_reward_over_time(file))\n",
    "        print(ids)\n",
    "        print(\"%.4f (%.3f)\" % (np.mean(average_reward_list), np.std(average_reward_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "020102\n",
      "0.7813 (0.058)\n",
      "020302\n",
      "0.7739 (0.140)\n",
      "020402\n",
      "0.7537 (0.145)\n",
      "020502\n",
      "0.7589 (0.144)\n"
     ]
    }
   ],
   "source": [
    "    # MLP level 2\n",
    "    average_reward_list = []\n",
    "    experiment_ids = [\n",
    "        \"020102\",\n",
    "        \"020302\",\n",
    "        \"020402\",\n",
    "        \"020502\"\n",
    "    ]\n",
    "    for ids in experiment_ids:\n",
    "        average_reward_list = []\n",
    "        for j in range(9):\n",
    "            file = '../experiments/experiment_' + str(ids) + '/repetition_00000' + str(j) + '/data/rollout_ep_rew_mean.npy'\n",
    "            average_reward_list.append(average_reward_over_time(file))\n",
    "        print(ids)\n",
    "        print(\"%.4f (%.3f)\" % (np.mean(average_reward_list), np.std(average_reward_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../experiments/experiment_111012/repetition_000000/data/rollout_ep_rew_mean.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m9\u001b[39m):\n\u001b[1;32m     12\u001b[0m     file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../experiments/experiment_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(ids) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/repetition_00000\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(j) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/data/rollout_ep_rew_mean.npy\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 13\u001b[0m     average_reward_list\u001b[38;5;241m.\u001b[39mappend(\u001b[43maverage_reward_over_time\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(ids)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%.4f\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m%.3f\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (np\u001b[38;5;241m.\u001b[39mmean(average_reward_list), np\u001b[38;5;241m.\u001b[39mstd(average_reward_list)))\n",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36maverage_reward_over_time\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maverage_reward_over_time\u001b[39m(file):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# read the data from the .npy file\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# min-max normalize the data\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     data \u001b[38;5;241m=\u001b[39m (data \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m200\u001b[39m) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m11\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/mini/lib/python3.8/site-packages/numpy/lib/npyio.py:390\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    388\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 390\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    391\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../experiments/experiment_111012/repetition_000000/data/rollout_ep_rew_mean.npy'"
     ]
    }
   ],
   "source": [
    "    # URBF level 2\n",
    "    average_reward_list = []\n",
    "    experiment_ids = [\n",
    "        \"111012\",\n",
    "        \"111112\",\n",
    "        \"111312\",\n",
    "        \"111412\"\n",
    "    ]\n",
    "    for ids in experiment_ids:\n",
    "        average_reward_list = []\n",
    "        for j in range(9):\n",
    "            file = '../experiments/experiment_' + str(ids) + '/repetition_00000' + str(j) + '/data/rollout_ep_rew_mean.npy'\n",
    "            average_reward_list.append(average_reward_over_time(file))\n",
    "        print(ids)\n",
    "        print(\"%.4f (%.3f)\" % (np.mean(average_reward_list), np.std(average_reward_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "020103\n",
      "nan (nan)\n",
      "020303\n",
      "0.5953 (0.102)\n",
      "020403\n",
      "0.6482 (0.141)\n",
      "020503\n",
      "0.6101 (0.121)\n"
     ]
    }
   ],
   "source": [
    "    # MLP Level 3\n",
    "    average_reward_list = []\n",
    "    experiment_ids = [\n",
    "        \"020103\",\n",
    "        \"020303\",\n",
    "        \"020403\",\n",
    "        \"020503\",\n",
    "    ]\n",
    "    for ids in experiment_ids:\n",
    "        average_reward_list = []\n",
    "        for j in range(9):\n",
    "            file = '../experiments/experiment_' + str(ids) + '/repetition_00000' + str(j) + '/data/rollout_ep_rew_mean.npy'\n",
    "            try:\n",
    "                average_reward_list.append(average_reward_over_time(file))\n",
    "            except:\n",
    "                continue\n",
    "        print(ids)\n",
    "        print(\"%.4f (%.3f)\" % (np.mean(average_reward_list), np.std(average_reward_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Total Return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "experiment_ids = [\n",
    "    #'00000',\n",
    "    '10300',\n",
    "    #'25300',\n",
    "    '60300',\n",
    "    '70300',\n",
    "    '80311',\n",
    "]\n",
    "\n",
    "labels = [\n",
    "    #'Random',\n",
    "    'QL', \n",
    "    #'SFQL (O)', \n",
    "    #'MF Xi', \n",
    "    #'MB Xi',\n",
    "    'SFQL', \n",
    "    'MF Xi', \n",
    "    'MB Xi'\n",
    "]\n",
    "\n",
    "# ranges = [\n",
    "#     '[0.0 0.25]',\n",
    "#     '[0.25 0.5]',\n",
    "#     '[0.5 0.75]',\n",
    "#     '[0.75 1.0]',\n",
    "#     '[1.0 1.25]',\n",
    "#     '[1.25 1.5]',\n",
    "#     '[1.5 1.75]',\n",
    "# ]\n",
    "\n",
    "ranges = [\n",
    "    '0.125',\n",
    "    '0.375',\n",
    "    '0.625',\n",
    "    '0.875',\n",
    "    '1.125',\n",
    "    '1.375',\n",
    "    '1.625',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# collect data \n",
    "collected_experiment_data = []\n",
    "\n",
    "for experiment_id_template in experiment_ids:\n",
    "    \n",
    "    cur_data = eu.AttrDict()\n",
    "    cur_data.means = []\n",
    "    cur_data.stds = []\n",
    "    #cur_data.sems = []\n",
    "    \n",
    "    for range_idx in range(len(ranges)):\n",
    "        \n",
    "        experiment_id = '{}{}'.format(experiment_id_template, range_idx)\n",
    "    \n",
    "        total_rewards = []\n",
    "        for rep_data in experiment_data_loader.experiment_data[experiment_id].repetition_data.values():\n",
    "            total_rewards.append(rep_data.total_reward[0])\n",
    "    \n",
    "        cur_data.means.append(np.mean(total_rewards))\n",
    "        cur_data.stds.append(np.std(total_rewards))\n",
    "        #cur_data.means.append(np.mean(total_rewards))\n",
    "        \n",
    "    collected_experiment_data.append(cur_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# plotting\n",
    "import plotly.graph_objects as go\n",
    "import plotly\n",
    "\n",
    "# default print properties\n",
    "multiplier = 2\n",
    "\n",
    "pixel_cm_ration = 36.5\n",
    "\n",
    "width_full = int(13.95 * pixel_cm_ration) * multiplier\n",
    "width_half = int(13.95/2 * pixel_cm_ration) * multiplier\n",
    "width_third = int(13.95/3 * pixel_cm_ration) * multiplier\n",
    "\n",
    "height_default_1 = int(3.5 * pixel_cm_ration) * multiplier\n",
    "\n",
    "# margins in pixel\n",
    "top_margin = 0 * multiplier \n",
    "left_margin = 35 * multiplier \n",
    "right_margin = 0 * multiplier \n",
    "bottom_margin = 25 * multiplier \n",
    "\n",
    "font_size = 8 * multiplier \n",
    "font_family='Times New Roman'\n",
    "\n",
    "line_width = 1 * multiplier \n",
    "\n",
    "layout = eu.AttrDict(\n",
    "        paper_bgcolor='rgba(0,0,0,0)',\n",
    "        plot_bgcolor='rgba(0,0,0,0)',\n",
    "        \n",
    "        xaxis = eu.AttrDict(\n",
    "            title = 'Mean Error of Linear Reward Model',\n",
    "            showline = True,\n",
    "            linewidth = 1,\n",
    "            zeroline=False,\n",
    "            linecolor='black',\n",
    "            showgrid=True,\n",
    "            gridwidth=1,\n",
    "            gridcolor='LightGrey',\n",
    "            mirror=True,\n",
    "            tickvals = np.arange(len(ranges)),\n",
    "            ticktext = ranges, \n",
    "            tickangle = 30, \n",
    "        ),\n",
    "        yaxis = eu.AttrDict(\n",
    "            title = 'Total Return',\n",
    "            showline = True,\n",
    "            linewidth = 1,\n",
    "            zeroline=False,\n",
    "            linecolor='black',\n",
    "            showgrid=True,\n",
    "            gridwidth=1,\n",
    "            gridcolor='LightGrey',\n",
    "            mirror=True,\n",
    "        ),\n",
    "        font = eu.AttrDict(\n",
    "            family=font_family, \n",
    "            size=font_size,\n",
    "            color='black',\n",
    "            ),\n",
    "        width=width_third, # in cm\n",
    "        height=height_default_1, # in cm\n",
    "        \n",
    "        margin = eu.AttrDict(\n",
    "            l=left_margin, #left margin in pixel\n",
    "            r=right_margin, #right margin in pixel\n",
    "            b=bottom_margin, #bottom margin in pixel\n",
    "            t=top_margin,  #top margin in pixel\n",
    "            ),\n",
    "\n",
    "        showlegend=True,\n",
    "        legend=eu.AttrDict(\n",
    "                orientation=\"h\",\n",
    "                yanchor=\"bottom\",\n",
    "                y=1.02,\n",
    "                xanchor=\"right\",\n",
    "                x=1,\n",
    "            \n",
    "            ), \n",
    "        )\n",
    "\n",
    "default_colors = [\n",
    "    'rgb(0,158,115)', # green\n",
    "    'rgb(0,0,0)', # black\n",
    "    #'rgb(0,0,0)', # black\n",
    "    'rgb(0,114,178)',  # dark blue\n",
    "    'rgb(213,94,0)',  # orange\n",
    "] \n",
    "\n",
    "fig = go.Figure(layout=layout)\n",
    "for exp_idx, exp_label in enumerate(labels):\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "            x=np.arange(len(ranges)),\n",
    "            y=collected_experiment_data[exp_idx].means,\n",
    "            error_y=dict(\n",
    "                type='data', # value of error bar given in data coordinates\n",
    "                array=collected_experiment_data[exp_idx].stds,\n",
    "                visible=True),\n",
    "            name= exp_label,\n",
    "            marker_color=default_colors[exp_idx]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    \n",
    "#plotly.io.write_image(fig, '../../analyze/iclr_figures/result_linear_model_task_difficulty_total_return.pdf')    \n",
    "# plotly.io.write_image(fig, './result_linear_model_task_difficulty_total_return.pdf')    \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "experiment_ids = [\n",
    "#     ('00000', '70300'),\n",
    "    ('10300', '70300'),\n",
    "    ('60300', '70300'),\n",
    "]\n",
    "\n",
    "labels = [\n",
    "#     'Random / MF Xi (R)',\n",
    "    'QL / Xi',\n",
    "    'SFQL / Xi',\n",
    "]\n",
    "\n",
    "# ranges = [\n",
    "#     '[0.0 0.25]',\n",
    "#     '[0.25 0.5]',\n",
    "#     '[0.5 0.75]',\n",
    "#     '[0.75 1.0]',\n",
    "#     '[1.0 1.25]',\n",
    "#     '[1.25 1.5]',\n",
    "#     '[1.5 1.75]',\n",
    "# ]\n",
    "\n",
    "ranges = [\n",
    "    '0.125',\n",
    "    '0.375',\n",
    "    '0.625',\n",
    "    '0.875',\n",
    "    '1.125',\n",
    "    '1.375',\n",
    "    '1.625',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# collect data \n",
    "collected_experiment_data = []\n",
    "\n",
    "for experiment_id_templates in experiment_ids:\n",
    "    \n",
    "    cur_data = eu.AttrDict()\n",
    "    cur_data.means = []\n",
    "    cur_data.stds = []\n",
    "    \n",
    "    for range_idx in range(len(ranges)):\n",
    "        \n",
    "        experiment_id_1 = '{}{}'.format(experiment_id_templates[0], range_idx)\n",
    "        experiment_id_2 = '{}{}'.format(experiment_id_templates[1], range_idx)\n",
    "        \n",
    "        ratios = []\n",
    "        for rep_idx in experiment_data_loader.experiment_data[experiment_id_1].repetition_data.keys():\n",
    "            \n",
    "            rep_data_1 = experiment_data_loader.experiment_data[experiment_id_1].repetition_data[rep_idx]\n",
    "            rep_data_2 = experiment_data_loader.experiment_data[experiment_id_2].repetition_data[rep_idx]\n",
    "            \n",
    "            ratios.append(rep_data_1.total_reward[0] / rep_data_2.total_reward[0])\n",
    "    \n",
    "        cur_data.means.append(np.mean(ratios))\n",
    "        cur_data.stds.append(np.std(ratios))\n",
    "         \n",
    "    collected_experiment_data.append(cur_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# plotting\n",
    "import plotly.graph_objects as go\n",
    "import plotly\n",
    "\n",
    "# default print properties\n",
    "multiplier = 2\n",
    "\n",
    "pixel_cm_ration = 36.5\n",
    "\n",
    "width_full = int(13.95 * pixel_cm_ration) * multiplier\n",
    "width_half = int(13.95/2 * pixel_cm_ration) * multiplier\n",
    "width_third = int(16/3 * pixel_cm_ration) * multiplier\n",
    "\n",
    "height_default_1 = int(3.4 * pixel_cm_ration) * multiplier\n",
    "\n",
    "# margins in pixel\n",
    "top_margin = 0 * multiplier \n",
    "left_margin = 10 * multiplier \n",
    "right_margin = 0 * multiplier \n",
    "bottom_margin = 25 * multiplier \n",
    "\n",
    "font_size = 8 * multiplier \n",
    "font_family='Times New Roman'\n",
    "\n",
    "line_width = 1 * multiplier \n",
    "\n",
    "layout = eu.AttrDict(\n",
    "        paper_bgcolor='rgba(0,0,0,0)',\n",
    "        plot_bgcolor='rgba(0,0,0,0)',\n",
    "        \n",
    "        xaxis = eu.AttrDict(\n",
    "            title = 'Mean Error of Linear Model',\n",
    "            showline = True,\n",
    "            linewidth = 1,\n",
    "            zeroline=False,\n",
    "            linecolor='black',\n",
    "            showgrid=True,\n",
    "            gridwidth=1,\n",
    "            gridcolor='LightGrey',\n",
    "            mirror=True,\n",
    "            tickvals = np.arange(len(ranges)),\n",
    "            ticktext = ranges,  \n",
    "            tickangle = 30, \n",
    "        ),\n",
    "        yaxis = eu.AttrDict(\n",
    "            title = 'Total Return Ratio',\n",
    "            showline = True,\n",
    "            linewidth = 1,\n",
    "            zeroline=False,\n",
    "            linecolor='black',\n",
    "            showgrid=True,\n",
    "            gridwidth=1,\n",
    "            gridcolor='LightGrey',\n",
    "            mirror=True,\n",
    "        ),\n",
    "        font = eu.AttrDict(\n",
    "            family=font_family, \n",
    "            size=font_size,\n",
    "            color='black',\n",
    "            ),\n",
    "        width=width_third, # in cm\n",
    "        height=height_default_1, # in cm\n",
    "        \n",
    "        margin = eu.AttrDict(\n",
    "            l=left_margin, #left margin in pixel\n",
    "            r=right_margin, #right margin in pixel\n",
    "            b=bottom_margin, #bottom margin in pixel\n",
    "            t=top_margin,  #top margin in pixel\n",
    "            ),\n",
    "\n",
    "        showlegend=True,\n",
    "        legend=eu.AttrDict(\n",
    "                orientation=\"h\",\n",
    "                yanchor=\"bottom\",\n",
    "                y=1.02,\n",
    "                xanchor=\"right\",\n",
    "                x=1,\n",
    "            \n",
    "            ), \n",
    "        )\n",
    "\n",
    "default_colors = [\n",
    "    'rgb(0,0,0)', # black\n",
    "    'rgb(0,114,178)',  # dark blue\n",
    "] \n",
    "\n",
    "\n",
    "fig = go.Figure(layout=layout)\n",
    "for exp_idx, exp_label in enumerate(labels):\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "            x=np.arange(len(ranges)),\n",
    "            y=collected_experiment_data[exp_idx].means,\n",
    "            error_y=dict(\n",
    "                type='data', # value of error bar given in data coordinates\n",
    "                array=collected_experiment_data[exp_idx].stds,\n",
    "                visible=True),\n",
    "            name= exp_label,\n",
    "            marker_color=default_colors[exp_idx]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    \n",
    "#plotly.io.write_image(fig, '../../analyze/iclr_figures/result_linear_model_task_difficulty_ratio.pdf') \n",
    "# plotly.io.write_image(fig, './result_linear_model_task_difficulty_ratio.pdf') \n",
    "plotly.io.write_image(fig, '/scratchlocal/creinke/data/study/perception/experiments/continuous_sf_01/analyze/icml_figures/result_linear_model_task_difficulty_ratio.pdf') \n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
