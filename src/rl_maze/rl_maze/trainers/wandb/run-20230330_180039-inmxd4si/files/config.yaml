wandb_version: 1

_current_progress_remaining:
  desc: null
  value: 1
_custom_logger:
  desc: null
  value: 'False'
_episode_num:
  desc: null
  value: 0
_episode_storage:
  desc: null
  value: None
_last_episode_starts:
  desc: null
  value: '[ True]'
_last_obs:
  desc: null
  value: "[[[1 1 1 1 1 1 1 1 1]\n  [1 4 0 0 0 0 0 2 1]\n  [1 0 0 0 0 0 0 0 1]\n  [1\
    \ 0 0 0 2 0 0 2 1]\n  [1 0 0 2 0 2 0 0 1]\n  [1 0 2 0 0 0 0 0 1]\n  [1 0 0 0 0\
    \ 2 0 0 1]\n  [1 0 0 0 0 0 0 3 1]\n  [1 1 1 1 1 1 1 1 1]]]"
_last_original_obs:
  desc: null
  value: None
_logger:
  desc: null
  value: <stable_baselines3.common.logger.Logger object at 0x7f3c393fd160>
_n_calls:
  desc: null
  value: 0
_n_updates:
  desc: null
  value: 0
_num_timesteps_at_start:
  desc: null
  value: 0
_total_timesteps:
  desc: null
  value: 20000
_vec_normalize_env:
  desc: null
  value: None
_wandb:
  desc: null
  value:
    cli_version: 0.14.0
    framework: torch
    is_jupyter_run: false
    is_kaggle_kernel: false
    python_version: 3.8.13
    start_time: 1680192039.967142
    t:
      1:
      - 1
      - 55
      2:
      - 1
      - 55
      3:
      - 22
      - 23
      4: 3.8.13
      5: 0.14.0
      8:
      - 5
action_noise:
  desc: null
  value: None
action_space:
  desc: null
  value: Discrete(4)
actor:
  desc: null
  value: None
algo:
  desc: null
  value: DQN
batch_size:
  desc: null
  value: 64
buffer_size:
  desc: null
  value: 100000
config:
  desc: null
  value: 'AttrDict({''size'': 9, ''seed'': 4900, ''difficulty'': 1, ''timesteps'':
    20000.0, ''net_arch'': [32, 128], ''lr'': 0.001, ''gamma'': 0.99, ''batch_size'':
    64, ''buffer_size'': 100000, ''exploration_initial_eps'': 1.0, ''exploration_fraction'':
    0.1, ''exploration_final_eps'': 0.02, ''rbf_mlp'': False, ''rbf_on'': False, ''mrbf_on'':
    True, ''mrbf_units'': 128, ''n_neurons_per_input'': 10, ''ranges'': [0, 10], ''latent_dim'':
    32, ''sutton_maze'': True, ''policy'': ''MLP''})'
device:
  desc: null
  value: cpu
env:
  desc: null
  value: <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f3c394045b0>
ep_info_buffer:
  desc: null
  value: deque([], maxlen=100)
ep_success_buffer:
  desc: null
  value: deque([], maxlen=100)
eval_env:
  desc: null
  value: None
exploration_final_eps:
  desc: null
  value: 0.02
exploration_fraction:
  desc: null
  value: 0.1
exploration_initial_eps:
  desc: null
  value: 1.0
exploration_rate:
  desc: null
  value: 0.0
exploration_schedule:
  desc: null
  value: <function get_linear_fn.<locals>.func at 0x7f3c3955f820>
gamma:
  desc: null
  value: 0.99
gradient_steps:
  desc: null
  value: 1
learning_rate:
  desc: null
  value: 0.001
learning_starts:
  desc: null
  value: 10000
lr_schedule:
  desc: null
  value: <function constant_fn.<locals>.func at 0x7f3c3955f8b0>
max_grad_norm:
  desc: null
  value: 10
n_envs:
  desc: null
  value: 1
num_timesteps:
  desc: null
  value: 0
observation_space:
  desc: null
  value: Box(0, 5, (9, 9), uint8)
optimize_memory_usage:
  desc: null
  value: 'False'
policy:
  desc: null
  value: "DQNPolicy(\n  (q_net): QNetwork(\n    (features_extractor): FlattenExtractor(\n\
    \      (flatten): Flatten(start_dim=1, end_dim=-1)\n    )\n    (q_net): Sequential(\n\
    \      (0): MRBF()\n      (1): Linear(in_features=128, out_features=32, bias=True)\n\
    \      (2): ReLU()\n      (3): Linear(in_features=32, out_features=128, bias=True)\n\
    \      (4): ReLU()\n      (5): Linear(in_features=128, out_features=4, bias=True)\n\
    \    )\n  )\n  (q_net_target): QNetwork(\n    (features_extractor): FlattenExtractor(\n\
    \      (flatten): Flatten(start_dim=1, end_dim=-1)\n    )\n    (q_net): Sequential(\n\
    \      (0): MRBF()\n      (1): Linear(in_features=128, out_features=32, bias=True)\n\
    \      (2): ReLU()\n      (3): Linear(in_features=32, out_features=128, bias=True)\n\
    \      (4): ReLU()\n      (5): Linear(in_features=128, out_features=4, bias=True)\n\
    \    )\n  )\n)"
policy_class:
  desc: null
  value: <class 'stable_baselines3.dqn.policies.DQNPolicy'>
policy_kwargs:
  desc: null
  value: '{}'
q_net:
  desc: null
  value: "QNetwork(\n  (features_extractor): FlattenExtractor(\n    (flatten): Flatten(start_dim=1,\
    \ end_dim=-1)\n  )\n  (q_net): Sequential(\n    (0): MRBF()\n    (1): Linear(in_features=128,\
    \ out_features=32, bias=True)\n    (2): ReLU()\n    (3): Linear(in_features=32,\
    \ out_features=128, bias=True)\n    (4): ReLU()\n    (5): Linear(in_features=128,\
    \ out_features=4, bias=True)\n  )\n)"
q_net_target:
  desc: null
  value: "QNetwork(\n  (features_extractor): FlattenExtractor(\n    (flatten): Flatten(start_dim=1,\
    \ end_dim=-1)\n  )\n  (q_net): Sequential(\n    (0): MRBF()\n    (1): Linear(in_features=128,\
    \ out_features=32, bias=True)\n    (2): ReLU()\n    (3): Linear(in_features=32,\
    \ out_features=128, bias=True)\n    (4): ReLU()\n    (5): Linear(in_features=128,\
    \ out_features=4, bias=True)\n  )\n)"
replay_buffer:
  desc: null
  value: <stable_baselines3.common.buffers.ReplayBuffer object at 0x7f3c394b6d30>
replay_buffer_class:
  desc: null
  value: <class 'stable_baselines3.common.buffers.ReplayBuffer'>
replay_buffer_kwargs:
  desc: null
  value: '{}'
reward_list:
  desc: null
  value: deque([], maxlen=1000)
sde_sample_freq:
  desc: null
  value: -1
seed:
  desc: null
  value: None
start_time:
  desc: null
  value: 1680192048056319399
target_update_interval:
  desc: null
  value: 10000
tau:
  desc: null
  value: 1.0
tensorboard_log:
  desc: null
  value: None
torchinfo:
  desc: null
  value: 'True'
train_freq:
  desc: null
  value: 'TrainFreq(frequency=4, unit=<TrainFrequencyUnit.STEP: ''step''>)'
use_sde:
  desc: null
  value: 'False'
use_sde_at_warmup:
  desc: null
  value: 'False'
verbose:
  desc: null
  value: 0
